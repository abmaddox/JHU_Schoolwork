{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee22622",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Perceptron - learns by adjusting weights applied to features that get adusted based on the outcome of a training point. It is best for linearly separable data, but can be adjusted to handle non-linear to a degree. 'Learning rate' and 'epochs' are hyperparameters. Feature scaling is necessary, numerical data is best\n",
    "\n",
    "* SVM - Perceptron's cooler brother. Tries to find an axis that provides the largest boundary between classes rather than minimize error. It can also be applied to non-linearly separated data. Hyperparameter C controls how much misclassification influences the decision boundary. Feature scaling is necessary, numerical data is best\n",
    "\n",
    "* Decision Tree - Good model for interpretation and visualization. Classification occurs in a step-by-step manner where the feature resulting in the most correct classifications is selected at each step. Hyperparameter 'depth' controls the amount of levels a tree has. Too much depth leads to overfitting. Feature scaling is not necessary, nominal data or numerical\n",
    "\n",
    "* Random Forest - a type of ensemble method that uses many decision trees with randomly selected training data (called a bootstrap sample), randomly selected features for classification, and large depth to obtain a decision by majority vote by the many trees. Each individual tree may suffer from overfitting but the final classification depends on the result from all trees and is beneficial for preventing overfitting. The number of trees 'k' is the most common hyperparameter, which is positively correlated to efficacy of the model but also positively correlated to computational cost. The size of the bootstrap sample and number of features for each tree may also be changed from defaults. Feature scaling not necessary, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18ac8b",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3920a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Dataset\n",
      "Rows:  307\n",
      "Features:  24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case #</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Description</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Tesla driver</th>\n",
       "      <th>Tesla occupant</th>\n",
       "      <th>Other vehicle</th>\n",
       "      <th>...</th>\n",
       "      <th>Verified Tesla Autopilot Deaths</th>\n",
       "      <th>Verified Tesla Autopilot Deaths + All Deaths Reported to NHTSA SGO</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Source</th>\n",
       "      <th>Note</th>\n",
       "      <th>Deceased 1</th>\n",
       "      <th>Deceased 2</th>\n",
       "      <th>Deceased 3</th>\n",
       "      <th>Deceased 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>1/17/2023</td>\n",
       "      <td>USA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Tesla crashes into back of semi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20230118162813/ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>1/7/2023</td>\n",
       "      <td>Canada</td>\n",
       "      <td>-</td>\n",
       "      <td>Tesla crashes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20230109041434/ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taren Singh Lal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>292.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>1/7/2023</td>\n",
       "      <td>USA</td>\n",
       "      <td>WA</td>\n",
       "      <td>Tesla hits pole, catches on fire</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20230107232745/ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>12/22/2022</td>\n",
       "      <td>USA</td>\n",
       "      <td>GA</td>\n",
       "      <td>Tesla crashes and burns</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221222203930/ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>12/19/2022</td>\n",
       "      <td>Canada</td>\n",
       "      <td>-</td>\n",
       "      <td>Tesla crashes into storefront</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://web.archive.org/web/20221223203725/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221223203725/ht...</td>\n",
       "      <td>https://web.archive.org/web/20221223203725/ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case #    Year        Date  Country   State   \\\n",
       "0   294.0  2022.0   1/17/2023      USA       CA   \n",
       "1   293.0  2022.0    1/7/2023   Canada        -   \n",
       "2   292.0  2022.0    1/7/2023      USA       WA   \n",
       "3   291.0  2022.0  12/22/2022      USA       GA   \n",
       "4   290.0  2022.0  12/19/2022   Canada        -   \n",
       "\n",
       "                         Description    Deaths   Tesla driver   \\\n",
       "0    Tesla crashes into back of semi        1.0             1    \n",
       "1                      Tesla crashes        1.0             1    \n",
       "2   Tesla hits pole, catches on fire        1.0             -    \n",
       "3            Tesla crashes and burns        1.0             1    \n",
       "4      Tesla crashes into storefront        1.0             -    \n",
       "\n",
       "   Tesla occupant   Other vehicle   ...  Verified Tesla Autopilot Deaths   \\\n",
       "0               -               -   ...                                -    \n",
       "1               -               -   ...                                -    \n",
       "2               1               -   ...                                -    \n",
       "3               -               -   ...                                -    \n",
       "4               -               -   ...                                -    \n",
       "\n",
       "   Verified Tesla Autopilot Deaths + All Deaths Reported to NHTSA SGO   \\\n",
       "0                                                 -                      \n",
       "1                                                 -                      \n",
       "2                                                 -                      \n",
       "3                                                 -                      \n",
       "4                                                 -                      \n",
       "\n",
       "                                         Unnamed: 16  \\\n",
       "0   https://web.archive.org/web/20221222203930/ht...   \n",
       "1   https://web.archive.org/web/20221222203930/ht...   \n",
       "2   https://web.archive.org/web/20221222203930/ht...   \n",
       "3   https://web.archive.org/web/20221222203930/ht...   \n",
       "4   https://web.archive.org/web/20221223203725/ht...   \n",
       "\n",
       "                                         Unnamed: 17  \\\n",
       "0   https://web.archive.org/web/20221222203930/ht...   \n",
       "1   https://web.archive.org/web/20221222203930/ht...   \n",
       "2   https://web.archive.org/web/20221222203930/ht...   \n",
       "3   https://web.archive.org/web/20221222203930/ht...   \n",
       "4   https://web.archive.org/web/20221223203725/ht...   \n",
       "\n",
       "                                             Source   Note   \\\n",
       "0   https://web.archive.org/web/20230118162813/ht...    NaN   \n",
       "1   https://web.archive.org/web/20230109041434/ht...    NaN   \n",
       "2   https://web.archive.org/web/20230107232745/ht...    NaN   \n",
       "3   https://web.archive.org/web/20221222203930/ht...    NaN   \n",
       "4   https://web.archive.org/web/20221223203725/ht...    NaN   \n",
       "\n",
       "         Deceased 1   Deceased 2   Deceased 3   Deceased 4   \n",
       "0                NaN          NaN          NaN          NaN  \n",
       "1   Taren Singh Lal           NaN          NaN          NaN  \n",
       "2                NaN          NaN          NaN          NaN  \n",
       "3                NaN          NaN          NaN          NaN  \n",
       "4                NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "baseDir = 'C:/Users/abmad/Documents/School/JHU/Spring 2023/Applied Machine Learning/J Notebooks'\n",
    "\n",
    "#downloaded from Kaggle, Tesla Autonomous Deaths Updated 2023\n",
    "#https://www.kaggle.com/datasets/ibriiee/tesla-autonomous-deaths-data-updated-2023\n",
    "teslaDF = pd.read_csv(baseDir + '/Datasets/Tesla Deaths - Deaths.csv')\n",
    "\n",
    "print('Tesla Dataset')\n",
    "print('Rows: ', len(teslaDF))\n",
    "print('Features: ', len(teslaDF.columns))\n",
    "teslaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f1ce95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  5. ...  1.  0.  0.]\n",
      "  [ 0.  0. 13. ... 15.  5.  0.]\n",
      "  [ 0.  3. 15. ... 11.  8.  0.]\n",
      "  ...\n",
      "  [ 0.  4. 11. ... 12.  7.  0.]\n",
      "  [ 0.  2. 14. ... 12.  0.  0.]\n",
      "  [ 0.  0.  6. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  5.  0.  0.]\n",
      "  [ 0.  0.  0. ...  9.  0.  0.]\n",
      "  [ 0.  0.  3. ...  6.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  1. ...  6.  0.  0.]\n",
      "  [ 0.  0.  1. ...  6.  0.  0.]\n",
      "  [ 0.  0.  0. ... 10.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ... 12.  0.  0.]\n",
      "  [ 0.  0.  3. ... 14.  0.  0.]\n",
      "  [ 0.  0.  8. ... 16.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  9. 16. ...  0.  0.  0.]\n",
      "  [ 0.  3. 13. ... 11.  5.  0.]\n",
      "  [ 0.  0.  0. ... 16.  9.  0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.  0.  1. ...  1.  0.  0.]\n",
      "  [ 0.  0. 13. ...  2.  1.  0.]\n",
      "  [ 0.  0. 16. ... 16.  5.  0.]\n",
      "  ...\n",
      "  [ 0.  0. 16. ... 15.  0.  0.]\n",
      "  [ 0.  0. 15. ... 16.  0.  0.]\n",
      "  [ 0.  0.  2. ...  6.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  2. ...  0.  0.  0.]\n",
      "  [ 0.  0. 14. ... 15.  1.  0.]\n",
      "  [ 0.  4. 16. ... 16.  7.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  0. ... 16.  2.  0.]\n",
      "  [ 0.  0.  4. ... 16.  2.  0.]\n",
      "  [ 0.  0.  5. ... 12.  0.  0.]]\n",
      "\n",
      " [[ 0.  0. 10. ...  1.  0.  0.]\n",
      "  [ 0.  2. 16. ...  1.  0.  0.]\n",
      "  [ 0.  0. 15. ... 15.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  4. 16. ... 16.  6.  0.]\n",
      "  [ 0.  8. 16. ... 16.  8.  0.]\n",
      "  [ 0.  1.  8. ... 12.  1.  0.]]]\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "#same image dataset used in Module 1 class notebook\n",
    "nums = datasets.load_digits()\n",
    "print(nums.images)\n",
    "print(nums.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234d5d4",
   "metadata": {},
   "source": [
    "* Numerical - integers and floating point values, they can be binned to make nominal data or left as-is depending on the model\n",
    "    * year and number of deaths are floating point values in the Tesla data\n",
    "    \n",
    "    \n",
    "* Nominal - a non-integer classification with a string name. Can be made into numerical data via one-hot encoding or assigning each possible classification an index correlating to a key table of classifications\n",
    "    * Country is a nominal value in the Tesla dataset\n",
    "    \n",
    "    \n",
    "* Date - can be given in a variety of formats, describes a day, month, and year. Can be nominalized into a date range then one-hot encoded to be numerical data or it can be given a numerical value by establishing a 'zero' time and increasing by one for each day past that point\n",
    "    * Tesla data gives a date in the MM/DD/YYYY format\n",
    "    \n",
    "    \n",
    "* Text - a string of some length. If it is used for classifying datapoints somehow then it can be transformed into numerical data as described for nominal data\n",
    "    * Tesla data has a description feature and country feature which are valuable for different reasons. Description is best for human interpretation and exclusion from the model while the country is useful for both human interpretation and for the model depending on the dependent variable\n",
    "    \n",
    "    \n",
    "* Image - these can come in many different formats and require specific libraries to parse them into usable formats (Ref. 1). In the sklearn dataset for handwritten digits they describe processing a 32X32 bitmap into an 8X8 bitmap where integer values give an intensity of coloring for a set of pixels.\n",
    "    * The arrays of integers in the digits dataset represent black and white images of integers 0-9\n",
    "    \n",
    "    \n",
    "    \n",
    "* Dependent variable - This is the target of the dataset that the model is made to predict. It may be numerical or nominal but can be converted between the two.\n",
    "    * In the digits dataset the dependent variable was the classification of the digit image as an integer 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebfeaf",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Recall - The amount of positive classifications that were correct (Ref. 4)\n",
    "    * Measured by dividing true positives (TP) by true positives plus false negatives (FN). \n",
    "    \n",
    "    $$ Recall = \\frac{TP}{TP +FN}\\ $$\n",
    "    \n",
    "    \n",
    "\n",
    "* AUC ROC - The ROC curve plots recall against false positive rate for different thresholds of classification. AUC stands for 'area under the curve', and refers to taking the integral of the ROC plot to get a representation of overall performance in classification (Ref. 2, 3, 4).\n",
    "    * AUC is computed by taking the integral from 0 to 1 of the ROC plot\n",
    "    \n",
    "    \n",
    "\n",
    "* F1 score - This uses precision (the percentage of correct positives out of all positives) and recall to let you know if both values are high, meaning the model is performing well (Ref. 2, 3)\n",
    "    * Calculated using precision (P) and recall (R). \n",
    "    \n",
    "    $$F1 = \\frac{2}{\\frac{1}{P}\\+\\frac{1}{R}\\}\\$$ \n",
    "\n",
    "\n",
    "* Mean squared error - Used for regression models. Finds average squared distance between actual values and predicted values. Squaring can make outliers have a large impact so the square root of MSE may also be used.\n",
    "\n",
    "   $$\n",
    "   MSE=\\frac{1}{N}\\sum\\limits_{j=1}^{N}(y_{j}-\\hat{y}_{j})^{2}\n",
    "   $$\n",
    "   \n",
    "   \n",
    "* Adjusted $R^{2}$ - a modified $R^{2}$ designed to prevent overfit models from misleading analysts. It accounts for the number of datapoints (n) and the number of independent variables (k) to prevent false increase in score (Ref. 2).\n",
    "\n",
    "$$R^{2}_{a} = 1 - [(\\frac{n-1}{n-k-1})(1-R^{2})]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc074e5a",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36edffdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.810351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.827200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.792228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.690132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.645365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0.882413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.810351</td>\n",
       "      <td>0.792228</td>\n",
       "      <td>0.690132</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.645365</td>\n",
       "      <td>0.882413</td>\n",
       "      <td>0.545871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.827200           0.635376  0.613498   \n",
       "TOEFL Score         0.827200     1.000000           0.649799  0.644410   \n",
       "University Rating   0.635376     0.649799           1.000000  0.728024   \n",
       "SOP                 0.613498     0.644410           0.728024  1.000000   \n",
       "LOR                 0.524679     0.541563           0.608651  0.663707   \n",
       "CGPA                0.825878     0.810574           0.705254  0.712154   \n",
       "Research            0.563398     0.467012           0.427047  0.408116   \n",
       "Chance of Admit     0.810351     0.792228           0.690132  0.684137   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.524679  0.825878  0.563398          0.810351  \n",
       "TOEFL Score        0.541563  0.810574  0.467012          0.792228  \n",
       "University Rating  0.608651  0.705254  0.427047          0.690132  \n",
       "SOP                0.663707  0.712154  0.408116          0.684137  \n",
       "LOR                1.000000  0.637469  0.372526          0.645365  \n",
       "CGPA               0.637469  1.000000  0.501311          0.882413  \n",
       "Research           0.372526  0.501311  1.000000          0.545871  \n",
       "Chance of Admit    0.645365  0.882413  0.545871          1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#simple average function\n",
    "def average(list1):\n",
    "    return sum(list1)/len(list1)\n",
    "\n",
    "#computes the numerator of the Pearson correlation coefficient equation\n",
    "def numerator(xAvg, xVals, yAvg, yVals):\n",
    "    sum = 0\n",
    "    for i in range(len(xVals)):\n",
    "        sum = sum + ((xVals[i]-xAvg)*(yVals[i]-yAvg))\n",
    "    return sum\n",
    "\n",
    "#computes the denominator of the Pearson correlation coefficient equation\n",
    "def denominator(xAvg, xVals, yAvg, yVals):\n",
    "    sumX = 0\n",
    "    sumY = 0\n",
    "    for i in range(len(xVals)):\n",
    "        sumX = sumX + pow((xVals[i]-xAvg),2)\n",
    "        sumY = sumY + pow((yVals[i]-yAvg),2)\n",
    "    \n",
    "    return math.sqrt(sumX) * math.sqrt(sumY)\n",
    "\n",
    "#computes a Pearson correlation coefficient for two sets of data\n",
    "def pearson(xVals, yVals):\n",
    "    xAvg = average(xVals)\n",
    "    yAvg = average(yVals)\n",
    "    \n",
    "    num = numerator(xAvg, xVals, yAvg, yVals)\n",
    "    denom = denominator(xAvg, xVals, yAvg, yVals)\n",
    "    return num/denom\n",
    "\n",
    "#original import of data downloaded from Kaggle\n",
    "dataDF = pd.read_csv(baseDir + '/Datasets/GradAdmissions/Admission_Predict_Ver1.1.csv')\n",
    "\n",
    "#removing the serial number column\n",
    "cleanDataDF = dataDF.drop('Serial No.', axis = 1)\n",
    "\n",
    "#converting to numpy array for easier manipulation\n",
    "data = cleanDataDF.to_numpy()\n",
    "\n",
    "#data is transposed such that data for each feature is separated by row\n",
    "data = np.transpose(data)\n",
    "\n",
    "pCorr = []\n",
    "\n",
    "#Calculates the Pearson coefficient for every row combination in the 'data' matrix\n",
    "for x in data:\n",
    "    temp = []\n",
    "    for y in data:\n",
    "        temp.append(pearson(x, y))\n",
    "    pCorr.append(temp)\n",
    "\n",
    "myPCorr = pd.DataFrame(data = pCorr,index = cleanDataDF.columns, columns = cleanDataDF.columns)\n",
    "myPCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4402aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.810351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.827200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.792228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.690132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.645365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0.882413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.810351</td>\n",
       "      <td>0.792228</td>\n",
       "      <td>0.690132</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.645365</td>\n",
       "      <td>0.882413</td>\n",
       "      <td>0.545871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.827200           0.635376  0.613498   \n",
       "TOEFL Score         0.827200     1.000000           0.649799  0.644410   \n",
       "University Rating   0.635376     0.649799           1.000000  0.728024   \n",
       "SOP                 0.613498     0.644410           0.728024  1.000000   \n",
       "LOR                 0.524679     0.541563           0.608651  0.663707   \n",
       "CGPA                0.825878     0.810574           0.705254  0.712154   \n",
       "Research            0.563398     0.467012           0.427047  0.408116   \n",
       "Chance of Admit     0.810351     0.792228           0.690132  0.684137   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.524679  0.825878  0.563398          0.810351  \n",
       "TOEFL Score        0.541563  0.810574  0.467012          0.792228  \n",
       "University Rating  0.608651  0.705254  0.427047          0.690132  \n",
       "SOP                0.663707  0.712154  0.408116          0.684137  \n",
       "LOR                1.000000  0.637469  0.372526          0.645365  \n",
       "CGPA               0.637469  1.000000  0.501311          0.882413  \n",
       "Research           0.372526  0.501311  1.000000          0.545871  \n",
       "Chance of Admit    0.645365  0.882413  0.545871          1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDataDF.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84f2b2",
   "metadata": {},
   "source": [
    "The diagonal is all 1 because that is the point where each feature is being compared to itself. There is total positive correlation because it is the same data. \n",
    "\n",
    "The correlation between independent variables can reveal relationships between them that mean they may not be truly independent from one another. For example, GRE scores and TOEFL scores are positively associated with GPA. This could be because a high GPA reflects understanding of course materials and those materials may be helpful on the exams. If two independent variables are highly correlated then it would be appropriate to remove one of them as well.\n",
    "\n",
    "The most important variable for prediciton should be the one with the highest correlation coefficient when used as an X value and chance of admission is the Y value. In this case, it appears that undergraduate GPA has the highest positive correlation with chance of admission, so I would assume that it has the most predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca322932",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) For image formats in machine learning\n",
    "\n",
    "https://stats.stackexchange.com/questions/440144/which-image-format-is-better-for-machine-learning-png-jpg-or-other\n",
    "\n",
    "2) Used for Question 3\n",
    "\n",
    "https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide\n",
    "\n",
    "3) Used for Question 3\n",
    "\n",
    "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_algorithms_performance_metrics.htm\n",
    "\n",
    "4) Used for Question 3\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "\n",
    "5) DataFrame documentation used to figure out how to manipulate the data \n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
